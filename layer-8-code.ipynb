{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# from google.colab import drive\n# drive.mount('/content/drive')","metadata":{"id":"23_9iwQd9ZoR","execution":{"iopub.status.busy":"2023-09-21T11:19:51.027747Z","iopub.execute_input":"2023-09-21T11:19:51.028302Z","iopub.status.idle":"2023-09-21T11:19:51.035340Z","shell.execute_reply.started":"2023-09-21T11:19:51.028255Z","shell.execute_reply":"2023-09-21T11:19:51.033712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# main","metadata":{"id":"3Ovraf6eOjmu"}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.feature_selection import SelectKBest, f_classif\nimport xgboost as xgb\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.svm import SVC\nimport matplotlib.pyplot as plt\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nLE=LabelEncoder()","metadata":{"id":"J-9Yu3uPEU47","execution":{"iopub.status.busy":"2023-09-21T11:19:51.037779Z","iopub.execute_input":"2023-09-21T11:19:51.038319Z","iopub.status.idle":"2023-09-21T11:19:51.051455Z","shell.execute_reply.started":"2023-09-21T11:19:51.038266Z","shell.execute_reply":"2023-09-21T11:19:51.050157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the dataset from CSV file\ntrain = pd.read_csv('/kaggle/input/layer-8/train.csv')\nvalid = pd.read_csv('/kaggle/input/layer-8/valid.csv')\ntest = pd.read_csv('/kaggle/input/layer-8/test.csv')","metadata":{"id":"hnOQeIDYEUi0","execution":{"iopub.status.busy":"2023-09-21T11:19:51.053211Z","iopub.execute_input":"2023-09-21T11:19:51.053582Z","iopub.status.idle":"2023-09-21T11:19:59.345207Z","shell.execute_reply.started":"2023-09-21T11:19:51.053551Z","shell.execute_reply":"2023-09-21T11:19:59.343579Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_original=train.copy()\nvalid_original=valid.copy()\ntest_original=test.copy()","metadata":{"id":"SNu50qPSHWg7","execution":{"iopub.status.busy":"2023-09-21T11:19:59.348311Z","iopub.execute_input":"2023-09-21T11:19:59.348759Z","iopub.status.idle":"2023-09-21T11:19:59.482656Z","shell.execute_reply.started":"2023-09-21T11:19:59.348724Z","shell.execute_reply":"2023-09-21T11:19:59.481066Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"id":"SeVtFuakzHHE","outputId":"69edb0e3-ef26-4dc6-b59d-2ce5d60e7002","execution":{"iopub.status.busy":"2023-09-21T11:19:59.484306Z","iopub.execute_input":"2023-09-21T11:19:59.484691Z","iopub.status.idle":"2023-09-21T11:19:59.516124Z","shell.execute_reply.started":"2023-09-21T11:19:59.484662Z","shell.execute_reply":"2023-09-21T11:19:59.514487Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.shape","metadata":{"id":"PAb9LN5yzfgG","outputId":"8cfa726b-10a4-4bab-ecf5-94d5fe255a28","execution":{"iopub.status.busy":"2023-09-21T11:19:59.518666Z","iopub.execute_input":"2023-09-21T11:19:59.519140Z","iopub.status.idle":"2023-09-21T11:19:59.528314Z","shell.execute_reply.started":"2023-09-21T11:19:59.519107Z","shell.execute_reply":"2023-09-21T11:19:59.526987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"TRAIN=[]\n\nfor i in range(4):\n    TRAIN.append(train.dropna(subset=[train.columns[768+i]]))\n\nTRAIN[0].shape, TRAIN[1].shape, TRAIN[2].shape, TRAIN[3].shape","metadata":{"id":"U7c4izY2zHHG","outputId":"582d55f1-cd7e-4fa7-e93d-3bc07b9a6865","execution":{"iopub.status.busy":"2023-09-21T11:19:59.532741Z","iopub.execute_input":"2023-09-21T11:19:59.533148Z","iopub.status.idle":"2023-09-21T11:20:00.040735Z","shell.execute_reply.started":"2023-09-21T11:19:59.533117Z","shell.execute_reply":"2023-09-21T11:20:00.039033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n\n# train_1=train.dropna(subset=[train.columns[768]])\n# train_2=train.dropna(subset=[train.columns[769]])\n# train_3=train.dropna(subset=[train.columns[770]])\n# train_4=train.dropna(subset=[train.columns[771]])","metadata":{"id":"56sjO7VWSpM4","execution":{"iopub.status.busy":"2023-09-21T11:20:00.042653Z","iopub.execute_input":"2023-09-21T11:20:00.043029Z","iopub.status.idle":"2023-09-21T11:20:00.049040Z","shell.execute_reply.started":"2023-09-21T11:20:00.042998Z","shell.execute_reply":"2023-09-21T11:20:00.047572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# train_1.shape, train_2.shape, train_3.shape,train_4.shape,","metadata":{"id":"OUz2cGoBT84u","execution":{"iopub.status.busy":"2023-09-21T11:20:00.051026Z","iopub.execute_input":"2023-09-21T11:20:00.051416Z","iopub.status.idle":"2023-09-21T11:20:00.064194Z","shell.execute_reply.started":"2023-09-21T11:20:00.051384Z","shell.execute_reply":"2023-09-21T11:20:00.062961Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"VALID=[]\n\nfor i in range(4):\n    VALID.append(valid.dropna(subset=[valid.columns[768+i]]))\n\nVALID[0].shape, VALID[1].shape, VALID[2].shape, VALID[3].shape","metadata":{"id":"mrjuh81mzHHI","outputId":"7ead2432-28f0-42c8-ad08-d372c404c6aa","execution":{"iopub.status.busy":"2023-09-21T11:20:00.069225Z","iopub.execute_input":"2023-09-21T11:20:00.069634Z","iopub.status.idle":"2023-09-21T11:20:00.103055Z","shell.execute_reply.started":"2023-09-21T11:20:00.069601Z","shell.execute_reply":"2023-09-21T11:20:00.101253Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# valid_1=valid.dropna(subset=[valid.columns[768]])\n# valid_2=valid.dropna(subset=[valid.columns[769]])\n# valid_3=valid.dropna(subset=[valid.columns[770]])\n# valid_4=valid.dropna(subset=[valid.columns[771]])","metadata":{"id":"41V7Dm8r5Ucx","execution":{"iopub.status.busy":"2023-09-21T11:20:00.105258Z","iopub.execute_input":"2023-09-21T11:20:00.105818Z","iopub.status.idle":"2023-09-21T11:20:00.112367Z","shell.execute_reply.started":"2023-09-21T11:20:00.105759Z","shell.execute_reply":"2023-09-21T11:20:00.110563Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"id":"kU75cA13EifG","outputId":"d02fffd6-770a-4f06-b997-d048165cba93","execution":{"iopub.status.busy":"2023-09-21T11:20:00.114285Z","iopub.execute_input":"2023-09-21T11:20:00.114828Z","iopub.status.idle":"2023-09-21T11:20:00.152895Z","shell.execute_reply.started":"2023-09-21T11:20:00.114791Z","shell.execute_reply":"2023-09-21T11:20:00.151490Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lable=\"y\"\nX_TRAIN=[]\nY_TRAIN=[]\n\nfor i in range(4):\n    X_TRAIN.append(TRAIN[i].iloc[:, :768])\n    Y_TRAIN.append(TRAIN[i].iloc[:, 768+i].to_frame(lable))\n\nX_TRAIN[0].shape, X_TRAIN[1].shape, X_TRAIN[2].shape, X_TRAIN[3].shape\n","metadata":{"id":"B6bYpEcFzHHJ","outputId":"8f2aea58-582a-45d2-b122-56cfc4808995","execution":{"iopub.status.busy":"2023-09-21T11:20:00.154725Z","iopub.execute_input":"2023-09-21T11:20:00.155140Z","iopub.status.idle":"2023-09-21T11:20:00.553628Z","shell.execute_reply.started":"2023-09-21T11:20:00.155105Z","shell.execute_reply":"2023-09-21T11:20:00.552574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lable=\"y\"\nX_VALID=[]\nY_VALID=[]\n\nfor i in range(4):\n    X_VALID.append(VALID[i].iloc[:, :768])\n    Y_VALID.append(VALID[i].iloc[:, 768+i].to_frame(lable))","metadata":{"id":"svbj5N08zHHJ","execution":{"iopub.status.busy":"2023-09-21T11:20:00.555127Z","iopub.execute_input":"2023-09-21T11:20:00.555731Z","iopub.status.idle":"2023-09-21T11:20:00.569845Z","shell.execute_reply.started":"2023-09-21T11:20:00.555697Z","shell.execute_reply":"2023-09-21T11:20:00.568412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_TRAIN[0].head()","metadata":{"id":"bQCGLmK4zHHK","outputId":"bb57265c-c71c-47be-cbfb-102779d00f1c","execution":{"iopub.status.busy":"2023-09-21T11:20:00.572087Z","iopub.execute_input":"2023-09-21T11:20:00.572671Z","iopub.status.idle":"2023-09-21T11:20:00.596103Z","shell.execute_reply.started":"2023-09-21T11:20:00.572622Z","shell.execute_reply":"2023-09-21T11:20:00.595065Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = test.iloc[:, 1:769]","metadata":{"id":"fFjtXKQ_zZBX","execution":{"iopub.status.busy":"2023-09-21T11:20:00.597946Z","iopub.execute_input":"2023-09-21T11:20:00.598955Z","iopub.status.idle":"2023-09-21T11:20:00.614905Z","shell.execute_reply.started":"2023-09-21T11:20:00.598898Z","shell.execute_reply":"2023-09-21T11:20:00.613132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test.head()","metadata":{"id":"fMz_4ygFzHHL","outputId":"42db6f1e-33bf-48b6-bb67-8e4e479da29b","execution":{"iopub.status.busy":"2023-09-21T11:20:00.616883Z","iopub.execute_input":"2023-09-21T11:20:00.617401Z","iopub.status.idle":"2023-09-21T11:20:00.661160Z","shell.execute_reply.started":"2023-09-21T11:20:00.617356Z","shell.execute_reply":"2023-09-21T11:20:00.660264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for i in range(4):\n  unique_classes, class_counts = np.unique(Y_TRAIN[i], return_counts=True)\n  plt.bar(unique_classes, class_counts)\n  plt.xlabel(f\"Label {i+1}\")\n  plt.ylabel('Number of samples')\n  plt.title('Label Distribution')\n  plt.show()","metadata":{"id":"ete7nIz3XgQs","outputId":"ad92ddfd-3b2c-4e09-db13-dbd3bcd7984f","execution":{"iopub.status.busy":"2023-09-21T11:20:00.662304Z","iopub.execute_input":"2023-09-21T11:20:00.663414Z","iopub.status.idle":"2023-09-21T11:20:02.067150Z","shell.execute_reply.started":"2023-09-21T11:20:00.663376Z","shell.execute_reply":"2023-09-21T11:20:02.065507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sampler = RandomOverSampler(random_state=45)\n# k = sampler.fit_resample(X_TRAIN[3], Y_TRAIN[3][lable])\n# X_TRAIN[3],Y_TRAIN[3]= k[0],k[1].to_frame(name=lable)\n# X_TRAIN[3].shape","metadata":{"id":"pZlHbY6MzHHM","outputId":"3be7bfbb-d979-4e97-dcca-bd478a19104c","execution":{"iopub.status.busy":"2023-09-21T11:20:02.069078Z","iopub.execute_input":"2023-09-21T11:20:02.069475Z","iopub.status.idle":"2023-09-21T11:20:02.076389Z","shell.execute_reply.started":"2023-09-21T11:20:02.069443Z","shell.execute_reply":"2023-09-21T11:20:02.074526Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from imblearn.under_sampling import RandomUnderSampler\n\n# # Assuming you have the following variables:\n# # X_TRAIN[3] is your feature data\n# # Y_TRAIN[3] is your target variable\n# # label is the label you want to balance\n\n# # Create a RandomUnderSampler instance\n# sampler = RandomUnderSampler(random_state=45)\n\n# # Fit and transform your data using the sampler\n# X_resampled, Y_resampled = sampler.fit_resample(X_TRAIN[3], Y_TRAIN[3][lable])\n# X_TRAIN[3],Y_TRAIN[3]=X_resampled, Y_resampled.to_frame(name=lable)\n\n# X_TRAIN[3].shape\n\n# # X_resampled and Y_resampled now contain the randomly undersampled data\n","metadata":{"id":"ER1pd5HtPD6D","execution":{"iopub.status.busy":"2023-09-21T11:20:02.078356Z","iopub.execute_input":"2023-09-21T11:20:02.079740Z","iopub.status.idle":"2023-09-21T11:20:02.091787Z","shell.execute_reply.started":"2023-09-21T11:20:02.079700Z","shell.execute_reply":"2023-09-21T11:20:02.090130Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# unique_classes, class_counts = np.unique(X_TRAIN[3], return_counts=True)\n# plt.bar(unique_classes, class_counts)\n# plt.xlabel(f\"Label {i+1}\")\n# plt.ylabel('Number of samples')\n# plt.title('Label Distribution')\n# plt.show()","metadata":{"id":"h3hsShkoj0jZ","execution":{"iopub.status.busy":"2023-09-21T11:20:02.093813Z","iopub.execute_input":"2023-09-21T11:20:02.094212Z","iopub.status.idle":"2023-09-21T11:20:02.105601Z","shell.execute_reply.started":"2023-09-21T11:20:02.094180Z","shell.execute_reply":"2023-09-21T11:20:02.104143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#XGBoost Classifier\ndef xgBoostModel(X_train,Y_train):\n  num_classes = len(Y_train[lable].unique())\n  if num_classes == 2:\n    objective = 'binary:logistic'\n  else:\n    objective = 'multi:softmax'\n  # Create an XGBoost model\n  model = xgb.XGBClassifier(objective=objective, random_state=39, tree_method='gpu_hist')\n  Y_train_encoded = LE.fit_transform(Y_train[lable])\n  # Train the model\n  model.fit(X_train, Y_train_encoded)\n  return model\n","metadata":{"id":"pRuEDianE_x2","execution":{"iopub.status.busy":"2023-09-21T11:20:02.107241Z","iopub.execute_input":"2023-09-21T11:20:02.108370Z","iopub.status.idle":"2023-09-21T11:20:02.120209Z","shell.execute_reply.started":"2023-09-21T11:20:02.108326Z","shell.execute_reply":"2023-09-21T11:20:02.118659Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Support Vector Classifier\ndef svmModel(X_train,Y_train):\n  # Create an Support Vector Classifier\n  model = SVC(kernel='rbf', decision_function_shape='ovr', random_state=40, C=100)\n  Y_train_encoded = LE.fit_transform(Y_train[lable])\n  # Train the model\n  model.fit(X_train, Y_train_encoded)\n  return model","metadata":{"id":"7OnLLYtjRT4X","execution":{"iopub.status.busy":"2023-09-21T11:20:40.754371Z","iopub.execute_input":"2023-09-21T11:20:40.755423Z","iopub.status.idle":"2023-09-21T11:20:40.763276Z","shell.execute_reply.started":"2023-09-21T11:20:40.755382Z","shell.execute_reply":"2023-09-21T11:20:40.761666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def evaluator(X_train,Y_train,X_valid,Y_valid,X_test,model,y_lable=\"1\" ):\n  model_name = model.__class__.__name__\n  print(model_name)\n\n  Y_pred_encoded = model.predict(X_valid)\n  Y_pred = LE.inverse_transform(Y_pred_encoded)\n\n  # Evaluate the y1 using valid data\n  accuracy = accuracy_score(Y_valid, Y_pred)\n  print(f\"Valid Data Accuracy for y{y_lable}: {accuracy:.2f}\")\n\n  # Test data the model using test data\n  Y_pred_test_encoded = model.predict(X_test)\n  Y_pred_test = LE.inverse_transform(Y_pred_test_encoded)\n\n  return Y_pred_test\n\n","metadata":{"id":"0enBaZucEGZt","execution":{"iopub.status.busy":"2023-09-21T11:20:02.134739Z","iopub.execute_input":"2023-09-21T11:20:02.135208Z","iopub.status.idle":"2023-09-21T11:20:02.152373Z","shell.execute_reply.started":"2023-09-21T11:20:02.135150Z","shell.execute_reply":"2023-09-21T11:20:02.151292Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# model_svm=svmModel(X_TRAIN[1],Y_TRAIN[1])\n# y_pred_after=evaluator(X_TRAIN[1],Y_TRAIN[1],X_VALID[1],Y_VALID[1],X_test,model_svm,\"1\")\n\nY_TEST_PRED=[]\n\nfor i in range(4):\n  print(f\"y{i+1} :\")\n  pca_model_svm=svmModel(X_TRAIN[i],Y_TRAIN[i])\n  y_pred_after=evaluator(X_TRAIN[i],Y_TRAIN[i],X_VALID[i],Y_VALID[i],X_test,pca_model_svm, f\"{i + 1}\")\n  Y_TEST_PRED.append(y_pred_after)\n\n","metadata":{"id":"RLo4goK1kDrx","outputId":"da4d4ef4-60ef-4240-806e-aaf3a49dbea6","execution":{"iopub.status.busy":"2023-09-21T11:20:51.635050Z","iopub.execute_input":"2023-09-21T11:20:51.635578Z","iopub.status.idle":"2023-09-21T11:31:40.600222Z","shell.execute_reply.started":"2023-09-21T11:20:51.635537Z","shell.execute_reply":"2023-09-21T11:31:40.598544Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Hyper parameter","metadata":{"id":"mewU7tKkyiV-"}},{"cell_type":"code","source":"# # hyperparameters tuning\n# from sklearn.model_selection import GridSearchCV\n# # hyperparameters tuning using random search\n# from sklearn.model_selection import RandomizedSearchCV\n\n# param_grid = {\n#     'C': [0.1, 1, 10],\n#     'kernel': ['linear', 'rbf', 'poly', 'sigmoid'],\n#     'gamma': ['scale', 'auto', 0.1, 1],\n#     # Add more hyperparameters and their values as needed\n# }\n\n# grid_search = RandomizedSearchCV(SVC(random_state=42), param_grid, cv=5, n_jobs=-1)\n# grid_search.fit(X_TRAIN[0],Y_TRAIN[0])\n\n# best_params = grid_search.best_params_\n# best_model = grid_search.best_estimator_\n\n# y_pred_1=evaluator(X_TRAIN[0],Y_TRAIN[0],X_VALID[0],Y_VALID[0],X_test,best_model)\n","metadata":{"id":"Nzk0ZnLnzHHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n\ndef evaluator1(X_train, Y_train, X_valid, Y_valid, X_test, model):\n    # Fit the model on the training data\n    model.fit(X_train, Y_train)\n\n    # Make predictions on the training, validation, and test sets\n    # y_train_pred = model.predict(X_train)\n    y_valid_pred = model.predict(X_valid)\n    y_test_pred = model.predict(X_test)\n\n    # Calculate evaluation metrics for each dataset\n    # train_accuracy = accuracy_score(Y_train, y_train_pred)\n    valid_accuracy = accuracy_score(Y_valid, y_valid_pred)\n    # test_accuracy = accuracy_score(Y_test, y_test_pred)\n\n    print(f\"Valid Data Accuracy for y1: {valid_accuracy:.2f}\")\n\n\n\n    return y_test_pred\n\n# Usage example:\n# results = evaluator(X_TRAIN[0], Y_TRAIN[0], X_VALID[0], Y_VALID[0], X_test, best_model)\n","metadata":{"id":"JJifpEWIVgXB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# from sklearn.model_selection import RandomizedSearchCV\n# from sklearn.svm import SVC\n# import numpy as np\n\n# # Reduce the search space for hyperparameters\n# param_grid = {\n#     'C': [0.1, 1, 10],\n#     'kernel': ['linear'],\n#     'gamma': ['scale', 'auto', 0.1, 1],\n# }\n\n# # Reduce the number of iterations\n# n_iter = 20\n\n# # Use a smaller subset of data for initial search\n# subset_indices = np.random.choice(len(X_TRAIN[0]), size=1000, replace=False)\n# X_subset = X_TRAIN[0][:]\n# Y_subset = Y_TRAIN[0][:]\n\n# grid_search = RandomizedSearchCV(SVC(random_state=42), param_grid, cv=5, n_jobs=-1, n_iter=n_iter)\n# grid_search.fit(X_subset, Y_subset)\n\n# best_params = grid_search.best_params_\n# best_model = grid_search.best_estimator_\n\n# # Perform final evaluation on the full dataset\n# # y_pred_1 = evaluator1(X_TRAIN[0], Y_TRAIN[0], X_VALID[0], Y_VALID[0], X_test, best_model)\n","metadata":{"id":"ziTz5C_7zHHO","outputId":"4cb8ecef-6472-4dc1-b2ef-55c023dd7221"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# y_pred_1 = evaluator1(X_TRAIN[0], Y_TRAIN[0], X_VALID[0], Y_VALID[0], X_test, best_model)","metadata":{"id":"eT2gSo5oWQxc","outputId":"376b9e3d-c602-465b-9958-c4ef9c6c4d17"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# PCA\n","metadata":{"id":"zqqmcOjxyuGl"}},{"cell_type":"code","source":"from sklearn.decomposition import PCA\n\ndef pca(X_train,X_valid,X_test,desired_variance_ratio=0.95):\n  desired_variance_ratio = desired_variance_ratio\n  pca = PCA(n_components=desired_variance_ratio, svd_solver='full')\n\n  X_train_pca = pca.fit_transform(X_train)\n  X_valid_pca = pca.transform(X_valid)\n  X_test_pca = pca.transform(X_test)\n\n  return X_train_pca ,X_valid_pca ,X_test_pca\n","metadata":{"id":"VIrRCqjJzHHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nX_TRAIN_PCA=[]\nX_VALID_PCA=[]\nX_TEST_PCA=[]\n\nfor i in range(4):\n  X_TRAIN_PCA.append(pca(X_TRAIN[i],X_VALID[i],X_test)[0])\n  X_VALID_PCA.append(pca(X_TRAIN[i],X_VALID[i],X_test)[1])\n  X_TEST_PCA.append(pca(X_TRAIN[i],X_VALID[i],X_test)[2])\n\nfor i in range(4):\n  print(X_TRAIN_PCA[i].shape, X_VALID_PCA[i].shape, X_TEST_PCA[i].shape)\n","metadata":{"id":"JpLBm9VCzHHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# pca_model_svm_1=svmModel(X_TRAIN_PCA[0],Y_TRAIN[0])\n# y_pred_after_1=evaluator(X_TRAIN_PCA[0],Y_TRAIN[0],X_VALID_PCA[0],Y_VALID[0],X_TEST_PCA[0],pca_model_svm_1,\"1\")\nY_TEST_PRED=[]\n\nfor i in range(4):\n  print(f\"y{i+1} :\")\n  pca_model_svm=svmModel(X_TRAIN_PCA[i],Y_TRAIN[i])\n  y_pred_after=evaluator(X_TRAIN_PCA[i],Y_TRAIN[i],X_VALID_PCA[i],Y_VALID[i],X_TEST_PCA[i],pca_model_svm, f\"{i + 1}\")\n  Y_TEST_PRED.append(y_pred_after)\n","metadata":{"id":"7nEc-KWozHHO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#create the svm model for y1\n# model_svm_1=svmModel(X_TRAIN[0],Y_TRAIN[0])\n# y_pred_1=evaluator(X_TRAIN[0],Y_TRAIN[0],X_VALID[0],Y_VALID[0],X_test,model_svm_1)\n\n#create the XGBoost model for y1\n# model_XGBoost_1=xgBoostModel(X_train_1,Y_train_1)\n# y_pred_low=evaluator(X_train_1,Y_train_1,X_valid_1,Y_valid_1,X_test,model_XGBoost_1)\n\n","metadata":{"id":"pOksCMv0qOgt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# explainability","metadata":{"id":"Hk0PhXlAzS8Q"}},{"cell_type":"code","source":"model_svm_1=svmModel(X_TRAIN_PCA[0],Y_TRAIN[0])\ny_pred_after_1=evaluator(X_TRAIN_PCA[0],Y_TRAIN[0],X_VALID_PCA[0],Y_VALID[0],X_TEST_PCA[0],model_svm_1,f\" {i + 1}\")\n\ncoefficients = model_svm_1.coef_\nabsolute_coefficients = np.abs(coefficients)\nabsolute_coefficients","metadata":{"id":"wEqgtJTdFsrF","outputId":"7d927679-4441-44a0-a3e8-5ae428af99e0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"absolute_coefficients.shape","metadata":{"id":"fziWMgqdG6Ji","outputId":"d6841162-0d19-456d-98b5-e92c58cf4c8d"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_weights = []\nnum_classes = len(Y_TRAIN[0][lable].unique())\n\nfor class_X in range(num_classes):\n    for class_Y in range(class_X + 1, num_classes):\n        index = int(class_X * (2 * num_classes - class_X - 1) / 2 + class_Y - class_X - 1)\n        for feature_index, weight in enumerate(coefficients[index]):\n            absolute_weight = np.abs(weight)\n            if len(top_weights) < 20:\n                top_weights.append((absolute_weight, feature_index, class_X, class_Y, weight))\n            else:\n                min_absolute_weight = min(top_weights, key=lambda x: x[0])\n                if absolute_weight > min_absolute_weight[0]:\n                    min_index = top_weights.index(min_absolute_weight)\n                    top_weights[min_index] = (absolute_weight, feature_index, class_X, class_Y, weight)\n\ntop_weights.sort(reverse=True)\n# Loop through the top_weights list and print the top 20 weights with their details\nfor i, (absolute_weight, feature_index, class_X, class_Y, weight) in enumerate(top_weights):\n    print(f\"Top {i + 1} Weight: {weight:.2f} | Feature Index: {feature_index} | Class X: {class_X} | Class Y: {class_Y}\")\n","metadata":{"id":"wym9m0uDMGMW","outputId":"4756234b-4121-4709-cd45-31f23e863cb0"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Lable 2 (Nural Network)","metadata":{"id":"-2SsJrW6nOGD"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset","metadata":{"id":"8N04SsAfnPmB","execution":{"iopub.status.busy":"2023-09-21T11:59:59.574833Z","iopub.execute_input":"2023-09-21T11:59:59.575369Z","iopub.status.idle":"2023-09-21T12:00:01.753012Z","shell.execute_reply.started":"2023-09-21T11:59:59.575334Z","shell.execute_reply":"2023-09-21T12:00:01.751668Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(f\"data dataset shape {data.shape}\")\n# print(f\"# of missing values {data['label_2'].isna().sum()}\")\nprint(f\"# of labels {Y_TRAIN[1].value_counts().shape[0]}\")\nprint(f\"label summary\\n{Y_TRAIN[1].value_counts()}\")\n\n# data.dropna(subset=['label_2'], inplace=True)\n# print(f\"data dataset shape {data.shape}\")","metadata":{"id":"-wQHNVmPnfPL","outputId":"597a1e9c-d775-4136-dbc8-807cc5c073a0","execution":{"iopub.status.busy":"2023-09-21T12:00:03.408673Z","iopub.execute_input":"2023-09-21T12:00:03.409422Z","iopub.status.idle":"2023-09-21T12:00:03.426050Z","shell.execute_reply.started":"2023-09-21T12:00:03.409383Z","shell.execute_reply":"2023-09-21T12:00:03.424276Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"VZLXgJ93oRvi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class AgeClassifier(nn.Module):\n    def __init__(self, dropout_prob=0.5, weight_decay=1e-5):\n        super(AgeClassifier, self).__init__()\n        self.linear1 = nn.Linear(768, 512)\n        self.linear2 = nn.Linear(512, 256)\n        self.linear3 = nn.Linear(256, 128)\n        self.linear4 = nn.Linear(128, 17)\n\n        # Define dropout layers\n        self.dropout1 = nn.Dropout(p=dropout_prob)\n        self.dropout2 = nn.Dropout(p=dropout_prob)\n        self.dropout3 = nn.Dropout(p=dropout_prob)\n\n        # Define L2 regularization (weight decay) for linear layers\n        self.l2_regularization = nn.Linear(1, 1)  # Initialize a linear layer with weight 1\n\n        # Set weight_decay for regularization\n        self.weight_decay = weight_decay\n\n    def forward(self, tensors):\n        output_l1 = torch.relu(self.linear1(tensors))\n        output_l2 = torch.relu(self.linear2(output_l1))\n        output_l3 = torch.relu(self.linear3(output_l2))\n        output_l4 = self.linear4(output_l3)\n        return output_l4\n\n    def l2_regularization_loss(self):\n        # Calculate L2 regularization loss for linear layers\n        l2_loss = 0.0\n        for param in self.parameters():\n            if param.requires_grad:\n                l2_loss += torch.norm(param, 2)\n        return self.weight_decay * l2_loss\n","metadata":{"id":"GbZtg7rgF9q-","execution":{"iopub.status.busy":"2023-09-21T12:00:06.234227Z","iopub.execute_input":"2023-09-21T12:00:06.234719Z","iopub.status.idle":"2023-09-21T12:00:06.248586Z","shell.execute_reply.started":"2023-09-21T12:00:06.234677Z","shell.execute_reply":"2023-09-21T12:00:06.247267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nlabel_2 = Y_TRAIN[1].values.reshape(-1, 1)\nohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False).fit(label_2)\nprint(ohe.categories_)\n\nlabel_2 = ohe.transform(label_2)\nprint(label_2)","metadata":{"id":"2yZPAA8FpgH2","outputId":"5a27c85e-1fed-4d36-f7c5-97f526e67fb5","execution":{"iopub.status.busy":"2023-09-21T12:00:21.685209Z","iopub.execute_input":"2023-09-21T12:00:21.685728Z","iopub.status.idle":"2023-09-21T12:00:21.709189Z","shell.execute_reply.started":"2023-09-21T12:00:21.685689Z","shell.execute_reply":"2023-09-21T12:00:21.707587Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\nX_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_TRAIN[1], label_2, test_size=0.2, stratify=label_2, random_state=2023)\n\n# X_train_tensors = torch.tensor(X_TRAIN[1].values, dtype=torch.float32)\n# print(X_train_tensors.shape)\n\n# X_test_tensors =  torch.tensor(X_VALID[1].values, dtype=torch.float32)\n# print(X_test_tensors.shape)\n\nX_train_tensors = torch.tensor(X_train_cv.iloc[:, :].values, dtype=torch.float32)\nprint(X_train_tensors.shape)\n\nX_test_tensors = torch.tensor(X_test_cv.iloc[:, :].values, dtype=torch.float32)\nprint(X_test_tensors.shape)","metadata":{"id":"eFRjhoaNp7JX","outputId":"d4e7e02e-b18c-48ba-d57d-e05be8c2c07e","execution":{"iopub.status.busy":"2023-09-21T12:01:40.506818Z","iopub.execute_input":"2023-09-21T12:01:40.507305Z","iopub.status.idle":"2023-09-21T12:01:41.531045Z","shell.execute_reply.started":"2023-09-21T12:01:40.507271Z","shell.execute_reply":"2023-09-21T12:01:41.529871Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"h7Kfh4jjrfJN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create an instance of the FakeBERT model\nageClassifier = AgeClassifier()\n\n# Define a cross-entropy loss function\ncriterion = nn.CrossEntropyLoss()\n\n# Create a DataLoader for batching\nbatch_size = 128\ndataset = TensorDataset(X_train_tensors, torch.tensor(y_train_cv))\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\n# Define an optimizer\noptimizer = torch.optim.Adam(ageClassifier.parameters(), lr=0.001)\n\n# Training loop\nepochs = 100\nfor epoch in range(epochs):\n\n    for inputs, targets in dataloader:\n        optimizer.zero_grad()\n        outputs = ageClassifier(inputs)\n\n        ce_loss = criterion(outputs, targets)\n        l2_loss = ageClassifier.l2_regularization_loss()\n\n        total_loss = ce_loss + l2_loss\n\n        total_loss.backward()\n        optimizer.step()\n\n    y_test_pred = ageClassifier(X_test_tensors)\n    ce = criterion(y_test_pred, torch.tensor(y_test_cv))\n    acc = (torch.argmax(y_test_pred, 1) == torch.argmax(torch.tensor(y_test_cv), 1)).float().mean()\n    print(f\"Epoch {epoch} validation: Cross-entropy={float(ce)}, Accuracy={float(acc)}\")\n\n# Save the trained model if needed\n# torch.save(ageClassifier.state_dict(), 'ageClassifier_model.pth')","metadata":{"id":"Con2HJ7iHp2g","outputId":"89fd2419-f6c1-4b2f-e083-321516a0e543","execution":{"iopub.status.busy":"2023-09-21T12:24:21.215723Z","iopub.execute_input":"2023-09-21T12:24:21.216263Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\n\nlabel_2=torch.tensor(Y_VALID[1].values, dtype=torch.float32)\nohe = OneHotEncoder(handle_unknown='ignore', sparse_output=False).fit(label_2)\nprint(ohe.categories_)\n\nlabel_2 = ohe.transform(label_2)\nprint(label_2)\nprint(label_2.shape)\n\n\n# Set your model to evaluation mode\nageClassifier.eval()\n\nX_valid_tensors = torch.tensor(X_VALID[1].values, dtype=torch.float32)\nprint(X_valid_tensors.shape)\n\n# label_2=torch.tensor(Y_VALID[1].values, dtype=torch.float32)\n# print(label_2.shape)\n\ny_pred = ageClassifier(X_valid_tensors)\nprint(y_pred.shape)\n\nce = criterion(y_pred, torch.tensor(label_2))\nacc = (torch.argmax(y_pred, 1) == torch.argmax(torch.tensor(label_2), 1)).float().mean()\nprint(f\"Cross-entropy={float(ce)}, Test Accuracy={float(acc)}\")","metadata":{"execution":{"iopub.status.busy":"2023-09-21T12:23:24.117584Z","iopub.execute_input":"2023-09-21T12:23:24.118144Z","iopub.status.idle":"2023-09-21T12:23:24.154334Z","shell.execute_reply.started":"2023-09-21T12:23:24.118100Z","shell.execute_reply":"2023-09-21T12:23:24.153361Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Generate CSV","metadata":{"id":"81p-Mlyjm-RQ"}},{"cell_type":"code","source":"# from sklearn.inspection import permutation_importance\n\n# # Calculate permutation importances\n# perm_importance = permutation_importance(model_svm_1,X_valid_1,Y_valid_1, n_repeats=30, random_state=42)\n# feature_importances = perm_importance.importances_mean","metadata":{"id":"_IccPm9lzHHP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"Y_TEST_PRED[0].shape","metadata":{"id":"anptbF_dEUK0","outputId":"c62f4a5e-1f4f-4529-c2d7-252982bebf60"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"id":"MhmAoq5nGt8J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def createCSVOutput(X_test_predict_array):\n  IDs=[i for i in range(1, X_test_predict_array[0].shape[0]+1)]\n\n  data = {\n    'ID': IDs,\n    'label_1': X_test_predict_array[0],\n    'label_2': X_test_predict_array[1],\n    'label_3': X_test_predict_array[2],\n    'label_4': X_test_predict_array[3],\n    # 'No of new features': [new_features.shape[1]] * len(y_pred),\n  }\n  # for i in range(new_features.shape[1]):\n  #   data[f'new_feature_{i+1}'] = new_features[:, i]\n  # for i in range(new_features.shape[1], 256):\n  #       data[f'new_feature_{i+1}'] = [0] * len(y_pred)\n  df = pd.DataFrame(data)\n  filename = f'/kaggle/working/190420V_layer_8_results.csv'\n  df.to_csv(filename, index=False)","metadata":{"id":"B6roP9I4Grsd","execution":{"iopub.status.busy":"2023-09-21T11:34:34.723084Z","iopub.execute_input":"2023-09-21T11:34:34.723871Z","iopub.status.idle":"2023-09-21T11:34:34.733857Z","shell.execute_reply.started":"2023-09-21T11:34:34.723810Z","shell.execute_reply":"2023-09-21T11:34:34.732869Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"createCSVOutput(Y_TEST_PRED)","metadata":{"id":"EVF7jIa6IMyM","execution":{"iopub.status.busy":"2023-09-21T11:34:39.584114Z","iopub.execute_input":"2023-09-21T11:34:39.584667Z","iopub.status.idle":"2023-09-21T11:34:39.602320Z","shell.execute_reply.started":"2023-09-21T11:34:39.584629Z","shell.execute_reply":"2023-09-21T11:34:39.600712Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{"id":"SjuteE2uMPwz"}}]}